{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYCIFAR10(Dataset):\n",
    "    def __init__(self, root, train=True, transforms=None, target_transforms=None):\n",
    "        self.cls = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship' ,'truck']\n",
    "        self.train = train\n",
    "        self.transforms = transforms\n",
    "        self.target_transforms = target_transforms\n",
    "        self.root = root\n",
    "        \n",
    "        # imgs:np.uint8, labels:np.int64\n",
    "        if self.train:\n",
    "            f1 = open(os.path.join(self.root, \"data_batch_1\"), \"rb\")\n",
    "            f2 = open(os.path.join(self.root, \"data_batch_2\"), \"rb\")\n",
    "            f3 = open(os.path.join(self.root, \"data_batch_3\"), \"rb\")\n",
    "            f4 = open(os.path.join(self.root, \"data_batch_4\"), \"rb\")\n",
    "            f5 = open(os.path.join(self.root, \"data_batch_5\"), \"rb\")\n",
    "            raw = f1.read() + f2.read() + f3.read() + f4.read() + f5.read()\n",
    "            \n",
    "            self.labels = []\n",
    "            self.data = []\n",
    "            for i in range(50000):\n",
    "                idx = i * (32 * 32 * 3 + 1)\n",
    "                labels_np = np.array(list(raw[idx:idx + 1]), dtype='int64')\n",
    "                data_np = np.array(list(raw[idx + 1:idx + 32 * 32 * 3 + 1]), dtype='uint8')\n",
    "                self.labels.append(labels_np)\n",
    "                self.data.append(data_np)\n",
    "            self.labels = np.concatenate(self.labels)\n",
    "            self.data = np.concatenate(self.data)\n",
    "            #print(self.data.shape)\n",
    "            #print(self.labels.shape)\n",
    "            \n",
    "            self.data = self.data.reshape((50000, 3, 32, 32))\n",
    "            self.data = self.data.transpose((0, 2, 3, 1))\n",
    "            #print(self.data.shape)\n",
    "            \n",
    "            f1.close()\n",
    "            f2.close()\n",
    "            f3.close()\n",
    "            f4.close()\n",
    "            f5.close()\n",
    "        else:\n",
    "            f1 = open(os.path.join(self.root, \"test_batch\"), \"rb\")\n",
    "            raw = f1.read()\n",
    "            \n",
    "            self.labels = []\n",
    "            self.data = []\n",
    "            for i in range(10000):\n",
    "                idx = i * (32 * 32 * 3 + 1)\n",
    "                labels_np = np.array(list(raw[idx:idx + 1]), dtype='int64')\n",
    "                data_np = np.array(list(raw[idx + 1:idx + 32 * 32 * 3 + 1]), dtype='uint8')\n",
    "                self.labels.append(labels_np)\n",
    "                self.data.append(data_np)\n",
    "            self.labels = np.concatenate(self.labels)\n",
    "            self.data = np.concatenate(self.data)\n",
    "            \n",
    "            self.data = self.data.reshape((10000, 3, 32, 32))\n",
    "            self.data = self.data.transpose((0, 2, 3, 1))\n",
    "            \n",
    "            f1.close()\n",
    "            \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return 50000\n",
    "        else:\n",
    "            return 10000\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], self.labels[idx]\n",
    "        #print(img.shape)\n",
    "        \n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        if self.target_transforms is not None:\n",
    "            target = self.target_transforms(target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def show(self):\n",
    "        if self.train:\n",
    "            for i in range(15):\n",
    "                plt.subplot(3, 5, i + 1)\n",
    "                idx = random.randint(0, 50000 - 1)\n",
    "                img, label = Image.fromarray(self.data[i]), self.labels[i]\n",
    "                print(label)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(img)\n",
    "                plt.title(self.cls[label])\n",
    "        else:\n",
    "            for i in range(15):\n",
    "                plt.subplot(3, 5, i + 1)\n",
    "                idx = random.randint(0, 10000 - 1)\n",
    "                img, label = Image.fromarray(self.data[i]), self.labels[i]\n",
    "                plt.axis('off')\n",
    "                plt.imshow(img)\n",
    "                plt.title(self.cls[label])\n",
    "        plt.show()\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "#dataset = CIFAR10(\"./data/cifar-10-batches-py/\", train=True, transforms=transform)\n",
    "#dataset.show()\n",
    "train_dataset = CIFAR10(\"./data\", train=True, download=False, transform=transform_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, num_workers=2, shuffle=True)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "test_dataset = CIFAR10(\"./data\", train=False, download=False, transform=transform_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (maxpool): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AvgPool2d(kernel_size=4, stride=1, padding=0)\n",
      "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def resnet():\n",
    "    net = models.resnet34(pretrained=False)\n",
    "    net.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) #(b,64,32,32)\n",
    "    net.bn1 = nn.BatchNorm2d(64)\n",
    "    net.maxpool = nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "    net.avgpool = nn.AvgPool2d(4, stride=1) #32/8\n",
    "    expansion = 1 #18,34 : expansion=1 else 4\n",
    "    net.fc = nn.Linear(512*expansion, 10) #\n",
    "    return net\n",
    "\n",
    "# torch.cuda.set_device(2)\n",
    "# CUDA_VISIBLE_DEVICES = 2\n",
    "# net = torch.nn.DataParallel(net, device_ids=[2])\n",
    "# net = Net.cuda(2)\n",
    "load = False\n",
    "gpu_en = True\n",
    "gpus = [2]\n",
    "net = resnet()\n",
    "if load:\n",
    "    net.load_state_dict(torch.load('net.pkl'))\n",
    "if gpu_en:\n",
    "    device = torch.device(\"cuda:{}\".format(gpus[0]) if torch.cuda.is_available() else \"cpu\")\n",
    "    \"\"\"\n",
    "    os.environment[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \"\"\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = torch.nn.DataParallel(net, device_ids=gpus)\n",
    "    net.to(device=gpus[0])\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epc:0,step:0,loss:2.529641,acc:0.109375\n",
      "epc:0,step:1,loss:2.856798,acc:0.187500\n",
      "epc:0,step:2,loss:2.866618,acc:0.140625\n",
      "epc:0,step:3,loss:2.871935,acc:0.156250\n",
      "epc:0,step:4,loss:2.433586,acc:0.109375\n",
      "epc:0,step:5,loss:3.054872,acc:0.125000\n",
      "epc:0,step:6,loss:2.285182,acc:0.171875\n",
      "epc:0,step:7,loss:2.488332,acc:0.203125\n",
      "epc:0,step:8,loss:2.324755,acc:0.140625\n",
      "epc:0,step:9,loss:2.410621,acc:0.171875\n",
      "epc:0,step:10,loss:2.431606,acc:0.109375\n",
      "epc:0,step:11,loss:2.429797,acc:0.281250\n",
      "epc:0,step:12,loss:2.318946,acc:0.234375\n",
      "epc:0,step:13,loss:2.049551,acc:0.265625\n",
      "epc:0,step:14,loss:2.199800,acc:0.093750\n",
      "epc:0,step:15,loss:2.163374,acc:0.250000\n",
      "epc:0,step:16,loss:2.321077,acc:0.171875\n",
      "epc:0,step:17,loss:2.007106,acc:0.203125\n",
      "epc:0,step:18,loss:2.076537,acc:0.187500\n",
      "epc:0,step:19,loss:2.124095,acc:0.218750\n",
      "epc:0,step:20,loss:2.074638,acc:0.265625\n",
      "epc:0,step:21,loss:2.268789,acc:0.296875\n",
      "epc:0,step:22,loss:2.101147,acc:0.218750\n",
      "epc:0,step:23,loss:2.216924,acc:0.171875\n",
      "epc:0,step:24,loss:2.118723,acc:0.218750\n",
      "epc:0,step:25,loss:1.975361,acc:0.171875\n",
      "epc:0,step:26,loss:2.061531,acc:0.281250\n",
      "epc:0,step:27,loss:1.988225,acc:0.265625\n",
      "epc:0,step:28,loss:2.114224,acc:0.171875\n",
      "epc:0,step:29,loss:2.041280,acc:0.171875\n",
      "epc:0,step:30,loss:2.021589,acc:0.265625\n",
      "epc:0,step:31,loss:2.051813,acc:0.281250\n",
      "epc:0,step:32,loss:2.009122,acc:0.250000\n",
      "epc:0,step:33,loss:1.968893,acc:0.312500\n",
      "epc:0,step:34,loss:1.899670,acc:0.265625\n",
      "epc:0,step:35,loss:2.031069,acc:0.281250\n",
      "epc:0,step:36,loss:2.075674,acc:0.187500\n",
      "epc:0,step:37,loss:2.073404,acc:0.281250\n",
      "epc:0,step:38,loss:1.881876,acc:0.234375\n",
      "epc:0,step:39,loss:1.787625,acc:0.375000\n",
      "epc:0,step:40,loss:2.002053,acc:0.343750\n",
      "epc:0,step:41,loss:1.941428,acc:0.265625\n",
      "epc:0,step:42,loss:1.864544,acc:0.250000\n",
      "epc:0,step:43,loss:1.938198,acc:0.312500\n",
      "epc:0,step:44,loss:2.027353,acc:0.265625\n",
      "epc:0,step:45,loss:2.252365,acc:0.203125\n",
      "epc:0,step:46,loss:1.783920,acc:0.359375\n",
      "epc:0,step:47,loss:1.947412,acc:0.234375\n",
      "epc:0,step:48,loss:1.972690,acc:0.328125\n",
      "epc:0,step:49,loss:1.984078,acc:0.281250\n",
      "epc:0,step:50,loss:1.740254,acc:0.265625\n",
      "epc:0,step:51,loss:1.835148,acc:0.312500\n",
      "epc:0,step:52,loss:1.780348,acc:0.328125\n",
      "epc:0,step:53,loss:1.991214,acc:0.250000\n",
      "epc:0,step:54,loss:1.910681,acc:0.296875\n",
      "epc:0,step:55,loss:2.001254,acc:0.250000\n",
      "epc:0,step:56,loss:1.856558,acc:0.296875\n",
      "epc:0,step:57,loss:2.016358,acc:0.312500\n",
      "epc:0,step:58,loss:2.005830,acc:0.281250\n",
      "epc:0,step:59,loss:1.958048,acc:0.203125\n",
      "epc:0,step:60,loss:1.869108,acc:0.281250\n",
      "epc:0,step:61,loss:1.856813,acc:0.265625\n",
      "epc:0,step:62,loss:1.908991,acc:0.218750\n",
      "epc:0,step:63,loss:1.820801,acc:0.312500\n",
      "epc:0,step:64,loss:2.001522,acc:0.250000\n",
      "epc:0,step:65,loss:1.912477,acc:0.218750\n",
      "epc:0,step:66,loss:2.220665,acc:0.156250\n",
      "epc:0,step:67,loss:2.066879,acc:0.234375\n",
      "epc:0,step:68,loss:1.781254,acc:0.359375\n",
      "epc:0,step:69,loss:1.901353,acc:0.265625\n",
      "epc:0,step:70,loss:2.069193,acc:0.250000\n",
      "epc:0,step:71,loss:2.095481,acc:0.218750\n",
      "epc:0,step:72,loss:1.940529,acc:0.265625\n",
      "epc:0,step:73,loss:1.816750,acc:0.296875\n",
      "epc:0,step:74,loss:1.859292,acc:0.328125\n",
      "epc:0,step:75,loss:1.734831,acc:0.406250\n",
      "epc:0,step:76,loss:1.887888,acc:0.328125\n",
      "epc:0,step:77,loss:1.886201,acc:0.250000\n",
      "epc:0,step:78,loss:1.839061,acc:0.296875\n",
      "epc:0,step:79,loss:1.853914,acc:0.312500\n",
      "epc:0,step:80,loss:1.884268,acc:0.281250\n",
      "epc:0,step:81,loss:2.007252,acc:0.234375\n",
      "epc:0,step:82,loss:2.027265,acc:0.250000\n",
      "epc:0,step:83,loss:1.950252,acc:0.218750\n",
      "epc:0,step:84,loss:1.730322,acc:0.375000\n",
      "epc:0,step:85,loss:1.764174,acc:0.312500\n",
      "epc:0,step:86,loss:1.764711,acc:0.343750\n",
      "epc:0,step:87,loss:1.805717,acc:0.328125\n",
      "epc:0,step:88,loss:1.823628,acc:0.281250\n",
      "epc:0,step:89,loss:1.781779,acc:0.171875\n",
      "epc:0,step:90,loss:1.821766,acc:0.328125\n",
      "epc:0,step:91,loss:1.812325,acc:0.265625\n",
      "epc:0,step:92,loss:1.676005,acc:0.312500\n",
      "epc:0,step:93,loss:1.852858,acc:0.312500\n",
      "epc:0,step:94,loss:1.664669,acc:0.406250\n",
      "epc:0,step:95,loss:1.805165,acc:0.343750\n",
      "epc:0,step:96,loss:1.725785,acc:0.296875\n",
      "epc:0,step:97,loss:1.794606,acc:0.375000\n",
      "epc:0,step:98,loss:1.925215,acc:0.265625\n",
      "epc:0,step:99,loss:1.655207,acc:0.343750\n",
      "epc:0,step:100,loss:1.913172,acc:0.250000\n",
      "epc:0,step:101,loss:1.785126,acc:0.265625\n",
      "epc:0,step:102,loss:2.117636,acc:0.250000\n",
      "epc:0,step:103,loss:1.876579,acc:0.343750\n",
      "epc:0,step:104,loss:1.809578,acc:0.328125\n",
      "epc:0,step:105,loss:1.804736,acc:0.343750\n",
      "epc:0,step:106,loss:1.851783,acc:0.250000\n",
      "epc:0,step:107,loss:1.778463,acc:0.328125\n",
      "epc:0,step:108,loss:1.873036,acc:0.296875\n",
      "epc:0,step:109,loss:1.901019,acc:0.265625\n",
      "epc:0,step:110,loss:1.838986,acc:0.296875\n",
      "epc:0,step:111,loss:1.886095,acc:0.281250\n",
      "epc:0,step:112,loss:1.871861,acc:0.312500\n",
      "epc:0,step:113,loss:1.735229,acc:0.343750\n",
      "epc:0,step:114,loss:1.932616,acc:0.265625\n",
      "epc:0,step:115,loss:1.873085,acc:0.296875\n",
      "epc:0,step:116,loss:1.859562,acc:0.343750\n",
      "epc:0,step:117,loss:1.767197,acc:0.375000\n",
      "epc:0,step:118,loss:1.742975,acc:0.312500\n",
      "epc:0,step:119,loss:1.989300,acc:0.312500\n",
      "epc:0,step:120,loss:1.829744,acc:0.343750\n",
      "epc:0,step:121,loss:1.788275,acc:0.281250\n",
      "epc:0,step:122,loss:1.808542,acc:0.328125\n",
      "epc:0,step:123,loss:2.208050,acc:0.156250\n",
      "epc:0,step:124,loss:1.900340,acc:0.312500\n",
      "epc:0,step:125,loss:1.617774,acc:0.406250\n",
      "epc:0,step:126,loss:1.854389,acc:0.265625\n",
      "epc:0,step:127,loss:1.727680,acc:0.281250\n",
      "epc:0,step:128,loss:1.863404,acc:0.312500\n",
      "epc:0,step:129,loss:1.879611,acc:0.265625\n",
      "epc:0,step:130,loss:1.774401,acc:0.296875\n",
      "epc:0,step:131,loss:1.917219,acc:0.265625\n",
      "epc:0,step:132,loss:1.746824,acc:0.375000\n",
      "epc:0,step:133,loss:1.846929,acc:0.328125\n",
      "epc:0,step:134,loss:1.862293,acc:0.343750\n",
      "epc:0,step:135,loss:1.679989,acc:0.296875\n",
      "epc:0,step:136,loss:1.764039,acc:0.312500\n",
      "epc:0,step:137,loss:1.914325,acc:0.281250\n",
      "epc:0,step:138,loss:1.783228,acc:0.265625\n",
      "epc:0,step:139,loss:1.894554,acc:0.343750\n",
      "epc:0,step:140,loss:1.609762,acc:0.359375\n",
      "epc:0,step:141,loss:1.948250,acc:0.234375\n",
      "epc:0,step:142,loss:1.665057,acc:0.375000\n",
      "epc:0,step:143,loss:1.702178,acc:0.296875\n",
      "epc:0,step:144,loss:1.948808,acc:0.296875\n",
      "epc:0,step:145,loss:1.715917,acc:0.296875\n",
      "epc:0,step:146,loss:1.926597,acc:0.375000\n",
      "epc:0,step:147,loss:1.791434,acc:0.203125\n",
      "epc:0,step:148,loss:1.607206,acc:0.421875\n",
      "epc:0,step:149,loss:1.738505,acc:0.281250\n",
      "epc:0,step:150,loss:1.805986,acc:0.328125\n",
      "epc:0,step:151,loss:1.713397,acc:0.359375\n",
      "epc:0,step:152,loss:1.629535,acc:0.421875\n",
      "epc:0,step:153,loss:1.921987,acc:0.296875\n",
      "epc:0,step:154,loss:1.751054,acc:0.359375\n",
      "epc:0,step:155,loss:1.697672,acc:0.328125\n",
      "epc:0,step:156,loss:1.630294,acc:0.250000\n",
      "epc:0,step:157,loss:1.753796,acc:0.421875\n",
      "epc:0,step:158,loss:1.853334,acc:0.250000\n",
      "epc:0,step:159,loss:1.834752,acc:0.218750\n",
      "epc:0,step:160,loss:1.814058,acc:0.390625\n",
      "epc:0,step:161,loss:1.574059,acc:0.390625\n",
      "epc:0,step:162,loss:1.656851,acc:0.437500\n",
      "epc:0,step:163,loss:1.479175,acc:0.453125\n",
      "epc:0,step:164,loss:1.833187,acc:0.312500\n",
      "epc:0,step:165,loss:1.634499,acc:0.359375\n",
      "epc:0,step:166,loss:1.832205,acc:0.250000\n",
      "epc:0,step:167,loss:1.764550,acc:0.343750\n",
      "epc:0,step:168,loss:1.938356,acc:0.234375\n",
      "epc:0,step:169,loss:1.658005,acc:0.359375\n",
      "epc:0,step:170,loss:1.663370,acc:0.390625\n",
      "epc:0,step:171,loss:1.808139,acc:0.312500\n",
      "epc:0,step:172,loss:1.674081,acc:0.343750\n",
      "epc:0,step:173,loss:1.836261,acc:0.343750\n",
      "epc:0,step:174,loss:2.000612,acc:0.296875\n",
      "epc:0,step:175,loss:1.755709,acc:0.406250\n",
      "epc:0,step:176,loss:1.467928,acc:0.421875\n",
      "epc:0,step:177,loss:1.599926,acc:0.390625\n",
      "epc:0,step:178,loss:1.864500,acc:0.250000\n",
      "epc:0,step:179,loss:1.760250,acc:0.406250\n",
      "epc:0,step:180,loss:1.671133,acc:0.390625\n",
      "epc:0,step:181,loss:1.918648,acc:0.343750\n",
      "epc:0,step:182,loss:1.569224,acc:0.359375\n",
      "epc:0,step:183,loss:1.782934,acc:0.234375\n",
      "epc:0,step:184,loss:1.802473,acc:0.312500\n",
      "epc:0,step:185,loss:1.644984,acc:0.375000\n",
      "epc:0,step:186,loss:1.834147,acc:0.359375\n",
      "epc:0,step:187,loss:1.776753,acc:0.328125\n",
      "epc:0,step:188,loss:1.635655,acc:0.343750\n",
      "epc:0,step:189,loss:1.366464,acc:0.468750\n",
      "epc:0,step:190,loss:1.972915,acc:0.171875\n",
      "epc:0,step:191,loss:1.701778,acc:0.359375\n",
      "epc:0,step:192,loss:1.637482,acc:0.390625\n",
      "epc:0,step:193,loss:1.661873,acc:0.359375\n",
      "epc:0,step:194,loss:1.551118,acc:0.484375\n",
      "epc:0,step:195,loss:1.763277,acc:0.328125\n",
      "epc:0,step:196,loss:2.146683,acc:0.265625\n",
      "epc:0,step:197,loss:1.906052,acc:0.421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epc:0,step:198,loss:1.508167,acc:0.437500\n",
      "epc:0,step:199,loss:1.587357,acc:0.453125\n",
      "epc:0,step:200,loss:1.653886,acc:0.343750\n",
      "epc:0,step:201,loss:1.553892,acc:0.359375\n",
      "epc:0,step:202,loss:1.680575,acc:0.390625\n",
      "epc:0,step:203,loss:1.662138,acc:0.343750\n",
      "epc:0,step:204,loss:1.790108,acc:0.375000\n",
      "epc:0,step:205,loss:1.631426,acc:0.406250\n",
      "epc:0,step:206,loss:1.790998,acc:0.375000\n",
      "epc:0,step:207,loss:1.829654,acc:0.312500\n",
      "epc:0,step:208,loss:1.611768,acc:0.375000\n",
      "epc:0,step:209,loss:1.629990,acc:0.437500\n",
      "epc:0,step:210,loss:1.656806,acc:0.390625\n",
      "epc:0,step:211,loss:1.779224,acc:0.343750\n",
      "epc:0,step:212,loss:1.507980,acc:0.421875\n",
      "epc:0,step:213,loss:1.653992,acc:0.312500\n",
      "epc:0,step:214,loss:1.728939,acc:0.359375\n",
      "epc:0,step:215,loss:1.889975,acc:0.296875\n",
      "epc:0,step:216,loss:1.834121,acc:0.375000\n",
      "epc:0,step:217,loss:1.604552,acc:0.359375\n",
      "epc:0,step:218,loss:1.633944,acc:0.437500\n",
      "epc:0,step:219,loss:1.459094,acc:0.406250\n",
      "epc:0,step:220,loss:1.648626,acc:0.437500\n",
      "epc:0,step:221,loss:1.800475,acc:0.359375\n",
      "epc:0,step:222,loss:1.492787,acc:0.421875\n",
      "epc:0,step:223,loss:1.396810,acc:0.468750\n",
      "epc:0,step:224,loss:1.852568,acc:0.406250\n",
      "epc:0,step:225,loss:1.732434,acc:0.296875\n",
      "epc:0,step:226,loss:1.649706,acc:0.296875\n",
      "epc:0,step:227,loss:1.726903,acc:0.312500\n",
      "epc:0,step:228,loss:1.664225,acc:0.343750\n",
      "epc:0,step:229,loss:1.665586,acc:0.328125\n",
      "epc:0,step:230,loss:1.669907,acc:0.328125\n",
      "epc:0,step:231,loss:1.624095,acc:0.468750\n",
      "epc:0,step:232,loss:1.702991,acc:0.359375\n",
      "epc:0,step:233,loss:1.729548,acc:0.437500\n",
      "epc:0,step:234,loss:1.864176,acc:0.390625\n",
      "epc:0,step:235,loss:1.676600,acc:0.421875\n",
      "epc:0,step:236,loss:1.568856,acc:0.390625\n",
      "epc:0,step:237,loss:1.857678,acc:0.296875\n",
      "epc:0,step:238,loss:1.575687,acc:0.406250\n",
      "epc:0,step:239,loss:1.558471,acc:0.453125\n",
      "epc:0,step:240,loss:1.594360,acc:0.406250\n",
      "epc:0,step:241,loss:1.744586,acc:0.281250\n",
      "epc:0,step:242,loss:1.577910,acc:0.421875\n",
      "epc:0,step:243,loss:1.685857,acc:0.375000\n",
      "epc:0,step:244,loss:1.532477,acc:0.468750\n",
      "epc:0,step:245,loss:1.634861,acc:0.328125\n",
      "epc:0,step:246,loss:1.574879,acc:0.359375\n",
      "epc:0,step:247,loss:1.620163,acc:0.312500\n",
      "epc:0,step:248,loss:1.675534,acc:0.421875\n",
      "epc:0,step:249,loss:1.710522,acc:0.312500\n",
      "epc:0,step:250,loss:1.632917,acc:0.375000\n",
      "epc:0,step:251,loss:1.346326,acc:0.484375\n",
      "epc:0,step:252,loss:1.558008,acc:0.359375\n",
      "epc:0,step:253,loss:1.820709,acc:0.296875\n",
      "epc:0,step:254,loss:1.646904,acc:0.375000\n",
      "epc:0,step:255,loss:1.716659,acc:0.375000\n",
      "epc:0,step:256,loss:1.569529,acc:0.468750\n",
      "epc:0,step:257,loss:1.563123,acc:0.406250\n",
      "epc:0,step:258,loss:1.666929,acc:0.265625\n",
      "epc:0,step:259,loss:1.674201,acc:0.343750\n",
      "epc:0,step:260,loss:1.480718,acc:0.359375\n",
      "epc:0,step:261,loss:1.529708,acc:0.343750\n",
      "epc:0,step:262,loss:1.649239,acc:0.390625\n",
      "epc:0,step:263,loss:1.520329,acc:0.468750\n",
      "epc:0,step:264,loss:1.675866,acc:0.359375\n",
      "epc:0,step:265,loss:1.771315,acc:0.406250\n",
      "epc:0,step:266,loss:1.507909,acc:0.453125\n",
      "epc:0,step:267,loss:1.482371,acc:0.609375\n",
      "epc:0,step:268,loss:1.708513,acc:0.343750\n",
      "epc:0,step:269,loss:1.640320,acc:0.390625\n",
      "epc:0,step:270,loss:1.864432,acc:0.343750\n",
      "epc:0,step:271,loss:1.796803,acc:0.390625\n",
      "epc:0,step:272,loss:1.508966,acc:0.500000\n",
      "epc:0,step:273,loss:1.917504,acc:0.296875\n",
      "epc:0,step:274,loss:1.506266,acc:0.437500\n",
      "epc:0,step:275,loss:1.504192,acc:0.421875\n",
      "epc:0,step:276,loss:1.422542,acc:0.500000\n",
      "epc:0,step:277,loss:1.421819,acc:0.437500\n",
      "epc:0,step:278,loss:1.581634,acc:0.468750\n",
      "epc:0,step:279,loss:1.658660,acc:0.390625\n",
      "epc:0,step:280,loss:1.471361,acc:0.468750\n",
      "epc:0,step:281,loss:1.543453,acc:0.468750\n",
      "epc:0,step:282,loss:1.596936,acc:0.375000\n",
      "epc:0,step:283,loss:1.845073,acc:0.359375\n",
      "epc:0,step:284,loss:1.573374,acc:0.421875\n",
      "epc:0,step:285,loss:1.683872,acc:0.328125\n",
      "epc:0,step:286,loss:1.451923,acc:0.453125\n",
      "epc:0,step:287,loss:1.407398,acc:0.437500\n",
      "epc:0,step:288,loss:1.576166,acc:0.406250\n",
      "epc:0,step:289,loss:1.535533,acc:0.484375\n",
      "epc:0,step:290,loss:1.910011,acc:0.359375\n",
      "epc:0,step:291,loss:1.586649,acc:0.390625\n",
      "epc:0,step:292,loss:1.523791,acc:0.359375\n",
      "epc:0,step:293,loss:1.566904,acc:0.406250\n",
      "epc:0,step:294,loss:1.426218,acc:0.531250\n",
      "epc:0,step:295,loss:1.866676,acc:0.437500\n",
      "epc:0,step:296,loss:1.732029,acc:0.250000\n",
      "epc:0,step:297,loss:1.888153,acc:0.265625\n",
      "epc:0,step:298,loss:1.609639,acc:0.437500\n",
      "epc:0,step:299,loss:1.639906,acc:0.359375\n",
      "epc:0,step:300,loss:1.713476,acc:0.406250\n",
      "epc:0,step:301,loss:1.535176,acc:0.375000\n",
      "epc:0,step:302,loss:1.437769,acc:0.562500\n",
      "epc:0,step:303,loss:1.567010,acc:0.390625\n",
      "epc:0,step:304,loss:1.376837,acc:0.500000\n",
      "epc:0,step:305,loss:1.628661,acc:0.468750\n",
      "epc:0,step:306,loss:1.711677,acc:0.421875\n",
      "epc:0,step:307,loss:1.520737,acc:0.390625\n",
      "epc:0,step:308,loss:1.437202,acc:0.437500\n",
      "epc:0,step:309,loss:1.590702,acc:0.421875\n",
      "epc:0,step:310,loss:1.329551,acc:0.515625\n",
      "epc:0,step:311,loss:1.438445,acc:0.500000\n",
      "epc:0,step:312,loss:1.696501,acc:0.375000\n",
      "epc:0,step:313,loss:1.520649,acc:0.484375\n",
      "epc:0,step:314,loss:1.670855,acc:0.375000\n",
      "epc:0,step:315,loss:1.806726,acc:0.375000\n",
      "epc:0,step:316,loss:1.564094,acc:0.468750\n",
      "epc:0,step:317,loss:1.692444,acc:0.390625\n",
      "epc:0,step:318,loss:1.444229,acc:0.546875\n",
      "epc:0,step:319,loss:1.660280,acc:0.390625\n",
      "epc:0,step:320,loss:1.560088,acc:0.390625\n",
      "epc:0,step:321,loss:1.472014,acc:0.406250\n",
      "epc:0,step:322,loss:1.436631,acc:0.531250\n",
      "epc:0,step:323,loss:1.391942,acc:0.515625\n",
      "epc:0,step:324,loss:1.631991,acc:0.328125\n",
      "epc:0,step:325,loss:1.283884,acc:0.593750\n",
      "epc:0,step:326,loss:1.575888,acc:0.453125\n",
      "epc:0,step:327,loss:1.597112,acc:0.406250\n",
      "epc:0,step:328,loss:1.681920,acc:0.328125\n",
      "epc:0,step:329,loss:1.414599,acc:0.437500\n",
      "epc:0,step:330,loss:1.676066,acc:0.359375\n",
      "epc:0,step:331,loss:1.747196,acc:0.359375\n",
      "epc:0,step:332,loss:1.374797,acc:0.437500\n",
      "epc:0,step:333,loss:1.696166,acc:0.406250\n",
      "epc:0,step:334,loss:1.454882,acc:0.500000\n",
      "epc:0,step:335,loss:1.464574,acc:0.453125\n",
      "epc:0,step:336,loss:1.704697,acc:0.437500\n",
      "epc:0,step:337,loss:1.645037,acc:0.390625\n",
      "epc:0,step:338,loss:1.612717,acc:0.390625\n",
      "epc:0,step:339,loss:1.505777,acc:0.406250\n",
      "epc:0,step:340,loss:1.402545,acc:0.437500\n",
      "epc:0,step:341,loss:1.798668,acc:0.421875\n",
      "epc:0,step:342,loss:1.657771,acc:0.406250\n",
      "epc:0,step:343,loss:1.515945,acc:0.359375\n",
      "epc:0,step:344,loss:1.519572,acc:0.453125\n",
      "epc:0,step:345,loss:1.636174,acc:0.406250\n",
      "epc:0,step:346,loss:1.636687,acc:0.390625\n",
      "epc:0,step:347,loss:1.560906,acc:0.390625\n",
      "epc:0,step:348,loss:1.882658,acc:0.343750\n",
      "epc:0,step:349,loss:1.237255,acc:0.484375\n",
      "epc:0,step:350,loss:1.532673,acc:0.453125\n",
      "epc:0,step:351,loss:1.467623,acc:0.406250\n",
      "epc:0,step:352,loss:1.733505,acc:0.343750\n",
      "epc:0,step:353,loss:1.629987,acc:0.359375\n",
      "epc:0,step:354,loss:1.484683,acc:0.390625\n",
      "epc:0,step:355,loss:1.289596,acc:0.562500\n",
      "epc:0,step:356,loss:1.664304,acc:0.421875\n",
      "epc:0,step:357,loss:1.457148,acc:0.421875\n",
      "epc:0,step:358,loss:1.495496,acc:0.359375\n",
      "epc:0,step:359,loss:1.646692,acc:0.468750\n",
      "epc:0,step:360,loss:1.648180,acc:0.343750\n",
      "epc:0,step:361,loss:1.465977,acc:0.515625\n",
      "epc:0,step:362,loss:1.518763,acc:0.453125\n",
      "epc:0,step:363,loss:1.482604,acc:0.484375\n",
      "epc:0,step:364,loss:1.696305,acc:0.421875\n",
      "epc:0,step:365,loss:1.192041,acc:0.593750\n",
      "epc:0,step:366,loss:1.614959,acc:0.390625\n",
      "epc:0,step:367,loss:1.631065,acc:0.406250\n",
      "epc:0,step:368,loss:1.646803,acc:0.406250\n",
      "epc:0,step:369,loss:1.500588,acc:0.453125\n",
      "epc:0,step:370,loss:1.786493,acc:0.421875\n",
      "epc:0,step:371,loss:1.278419,acc:0.531250\n",
      "epc:0,step:372,loss:1.466075,acc:0.375000\n",
      "epc:0,step:373,loss:1.658192,acc:0.390625\n",
      "epc:0,step:374,loss:1.336252,acc:0.484375\n",
      "epc:0,step:375,loss:1.376755,acc:0.484375\n",
      "epc:0,step:376,loss:1.589608,acc:0.390625\n",
      "epc:0,step:377,loss:1.797398,acc:0.375000\n",
      "epc:0,step:378,loss:1.182992,acc:0.546875\n",
      "epc:0,step:379,loss:1.754117,acc:0.453125\n",
      "epc:0,step:380,loss:1.624663,acc:0.328125\n",
      "epc:0,step:381,loss:1.604053,acc:0.406250\n",
      "epc:0,step:382,loss:1.623028,acc:0.468750\n",
      "epc:0,step:383,loss:1.479622,acc:0.437500\n",
      "epc:0,step:384,loss:1.815084,acc:0.312500\n",
      "epc:0,step:385,loss:1.513955,acc:0.390625\n",
      "epc:0,step:386,loss:1.401951,acc:0.531250\n",
      "epc:0,step:387,loss:1.596531,acc:0.343750\n",
      "epc:0,step:388,loss:1.387076,acc:0.500000\n",
      "epc:0,step:389,loss:1.540156,acc:0.468750\n",
      "epc:0,step:390,loss:1.355915,acc:0.515625\n",
      "epc:0,step:391,loss:1.555740,acc:0.375000\n",
      "epc:0,step:392,loss:1.330939,acc:0.515625\n",
      "epc:0,step:393,loss:1.509875,acc:0.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epc:0,step:394,loss:1.651608,acc:0.453125\n",
      "epc:0,step:395,loss:1.624426,acc:0.406250\n",
      "epc:0,step:396,loss:1.674324,acc:0.406250\n",
      "epc:0,step:397,loss:1.412560,acc:0.453125\n",
      "epc:0,step:398,loss:1.430778,acc:0.500000\n",
      "epc:0,step:399,loss:1.532195,acc:0.437500\n",
      "epc:0,step:400,loss:1.486508,acc:0.500000\n",
      "epc:0,step:401,loss:1.381965,acc:0.468750\n",
      "epc:0,step:402,loss:1.696637,acc:0.437500\n",
      "epc:0,step:403,loss:1.414034,acc:0.484375\n",
      "epc:0,step:404,loss:1.451959,acc:0.468750\n",
      "epc:0,step:405,loss:1.381572,acc:0.515625\n",
      "epc:0,step:406,loss:1.739638,acc:0.375000\n",
      "epc:0,step:407,loss:1.303762,acc:0.484375\n",
      "epc:0,step:408,loss:1.482362,acc:0.468750\n",
      "epc:0,step:409,loss:1.475715,acc:0.343750\n",
      "epc:0,step:410,loss:1.466106,acc:0.468750\n",
      "epc:0,step:411,loss:1.303799,acc:0.546875\n",
      "epc:0,step:412,loss:1.258653,acc:0.531250\n",
      "epc:0,step:413,loss:1.398704,acc:0.484375\n",
      "epc:0,step:414,loss:1.643488,acc:0.484375\n",
      "epc:0,step:415,loss:1.412202,acc:0.453125\n",
      "epc:0,step:416,loss:1.447120,acc:0.437500\n",
      "epc:0,step:417,loss:1.576023,acc:0.437500\n",
      "epc:0,step:418,loss:1.620210,acc:0.453125\n",
      "epc:0,step:419,loss:1.505134,acc:0.359375\n",
      "epc:0,step:420,loss:1.418495,acc:0.421875\n",
      "epc:0,step:421,loss:1.505739,acc:0.468750\n",
      "epc:0,step:422,loss:1.703393,acc:0.421875\n",
      "epc:0,step:423,loss:1.371382,acc:0.500000\n",
      "epc:0,step:424,loss:1.631648,acc:0.406250\n",
      "epc:0,step:425,loss:1.470406,acc:0.406250\n",
      "epc:0,step:426,loss:1.778970,acc:0.343750\n",
      "epc:0,step:427,loss:1.478039,acc:0.437500\n",
      "epc:0,step:428,loss:1.320114,acc:0.484375\n",
      "epc:0,step:429,loss:1.491651,acc:0.375000\n",
      "epc:0,step:430,loss:1.656515,acc:0.421875\n",
      "epc:0,step:431,loss:1.486719,acc:0.453125\n",
      "epc:0,step:432,loss:1.597403,acc:0.421875\n",
      "epc:0,step:433,loss:1.392139,acc:0.515625\n",
      "epc:0,step:434,loss:1.695889,acc:0.390625\n",
      "epc:0,step:435,loss:1.490915,acc:0.437500\n",
      "epc:0,step:436,loss:1.465396,acc:0.437500\n",
      "epc:0,step:437,loss:1.340978,acc:0.406250\n",
      "epc:0,step:438,loss:1.607172,acc:0.406250\n",
      "epc:0,step:439,loss:1.634438,acc:0.359375\n",
      "epc:0,step:440,loss:1.505273,acc:0.343750\n",
      "epc:0,step:441,loss:1.508261,acc:0.390625\n",
      "epc:0,step:442,loss:1.466831,acc:0.484375\n",
      "epc:0,step:443,loss:1.640643,acc:0.390625\n",
      "epc:0,step:444,loss:1.606384,acc:0.421875\n",
      "epc:0,step:445,loss:1.495424,acc:0.468750\n",
      "epc:0,step:446,loss:1.818396,acc:0.406250\n",
      "epc:0,step:447,loss:1.838974,acc:0.312500\n",
      "epc:0,step:448,loss:1.534704,acc:0.437500\n",
      "epc:0,step:449,loss:1.423112,acc:0.484375\n",
      "epc:0,step:450,loss:1.520742,acc:0.515625\n",
      "epc:0,step:451,loss:1.646815,acc:0.421875\n",
      "epc:0,step:452,loss:1.641776,acc:0.359375\n",
      "epc:0,step:453,loss:1.638332,acc:0.406250\n",
      "epc:0,step:454,loss:1.471127,acc:0.453125\n",
      "epc:0,step:455,loss:1.367257,acc:0.437500\n",
      "epc:0,step:456,loss:1.511671,acc:0.453125\n",
      "epc:0,step:457,loss:1.539815,acc:0.359375\n",
      "epc:0,step:458,loss:1.520006,acc:0.421875\n",
      "epc:0,step:459,loss:1.219955,acc:0.578125\n",
      "epc:0,step:460,loss:1.622264,acc:0.437500\n",
      "epc:0,step:461,loss:1.563052,acc:0.406250\n",
      "epc:0,step:462,loss:1.570210,acc:0.453125\n",
      "epc:0,step:463,loss:1.519156,acc:0.421875\n",
      "epc:0,step:464,loss:1.341657,acc:0.546875\n",
      "epc:0,step:465,loss:1.737856,acc:0.406250\n",
      "epc:0,step:466,loss:1.414888,acc:0.484375\n",
      "epc:0,step:467,loss:1.412841,acc:0.437500\n",
      "epc:0,step:468,loss:1.469869,acc:0.468750\n",
      "epc:0,step:469,loss:1.544849,acc:0.453125\n",
      "epc:0,step:470,loss:1.455846,acc:0.531250\n",
      "epc:0,step:471,loss:1.598758,acc:0.343750\n",
      "epc:0,step:472,loss:1.250450,acc:0.500000\n",
      "epc:0,step:473,loss:1.407076,acc:0.468750\n",
      "epc:0,step:474,loss:1.355100,acc:0.515625\n",
      "epc:0,step:475,loss:1.353796,acc:0.500000\n",
      "epc:0,step:476,loss:1.343163,acc:0.500000\n",
      "epc:0,step:477,loss:1.791078,acc:0.406250\n",
      "epc:0,step:478,loss:1.362710,acc:0.484375\n",
      "epc:0,step:479,loss:1.207265,acc:0.484375\n",
      "epc:0,step:480,loss:1.484751,acc:0.468750\n",
      "epc:0,step:481,loss:1.736178,acc:0.375000\n",
      "epc:0,step:482,loss:1.400402,acc:0.437500\n",
      "epc:0,step:483,loss:1.567609,acc:0.531250\n",
      "epc:0,step:484,loss:1.505438,acc:0.531250\n",
      "epc:0,step:485,loss:1.637702,acc:0.375000\n",
      "epc:0,step:486,loss:1.496627,acc:0.500000\n",
      "epc:0,step:487,loss:1.532085,acc:0.468750\n",
      "epc:0,step:488,loss:1.322508,acc:0.484375\n",
      "epc:0,step:489,loss:1.503499,acc:0.421875\n",
      "epc:0,step:490,loss:1.325963,acc:0.531250\n",
      "epc:0,step:491,loss:1.477506,acc:0.421875\n",
      "epc:0,step:492,loss:1.399054,acc:0.484375\n",
      "epc:0,step:493,loss:1.378616,acc:0.468750\n",
      "epc:0,step:494,loss:1.161531,acc:0.593750\n",
      "epc:0,step:495,loss:1.318947,acc:0.406250\n",
      "epc:0,step:496,loss:1.602684,acc:0.484375\n",
      "epc:0,step:497,loss:1.346199,acc:0.546875\n",
      "epc:0,step:498,loss:1.268656,acc:0.546875\n",
      "epc:0,step:499,loss:1.362979,acc:0.562500\n",
      "epc:0,step:500,loss:1.288770,acc:0.500000\n",
      "epc:0,step:501,loss:1.411023,acc:0.500000\n",
      "epc:0,step:502,loss:1.384303,acc:0.484375\n",
      "epc:0,step:503,loss:1.418303,acc:0.531250\n",
      "epc:0,step:504,loss:1.038133,acc:0.546875\n",
      "epc:0,step:505,loss:1.514384,acc:0.484375\n",
      "epc:0,step:506,loss:1.581120,acc:0.468750\n",
      "epc:0,step:507,loss:1.230617,acc:0.500000\n",
      "epc:0,step:508,loss:1.354991,acc:0.500000\n",
      "epc:0,step:509,loss:1.228253,acc:0.484375\n",
      "epc:0,step:510,loss:1.428062,acc:0.484375\n",
      "epc:0,step:511,loss:1.234899,acc:0.546875\n",
      "epc:0,step:512,loss:1.540290,acc:0.546875\n",
      "epc:0,step:513,loss:1.312816,acc:0.484375\n",
      "epc:0,step:514,loss:1.514920,acc:0.453125\n",
      "epc:0,step:515,loss:1.498297,acc:0.515625\n",
      "epc:0,step:516,loss:1.452742,acc:0.437500\n",
      "epc:0,step:517,loss:1.529879,acc:0.421875\n",
      "epc:0,step:518,loss:1.439841,acc:0.453125\n",
      "epc:0,step:519,loss:1.489790,acc:0.468750\n",
      "epc:0,step:520,loss:1.284700,acc:0.500000\n",
      "epc:0,step:521,loss:1.508242,acc:0.406250\n",
      "epc:0,step:522,loss:1.413473,acc:0.437500\n",
      "epc:0,step:523,loss:1.627055,acc:0.453125\n",
      "epc:0,step:524,loss:1.480193,acc:0.421875\n",
      "epc:0,step:525,loss:1.644257,acc:0.375000\n",
      "epc:0,step:526,loss:1.493383,acc:0.437500\n",
      "epc:0,step:527,loss:1.301744,acc:0.546875\n",
      "epc:0,step:528,loss:1.457216,acc:0.390625\n",
      "epc:0,step:529,loss:1.489629,acc:0.453125\n",
      "epc:0,step:530,loss:1.442932,acc:0.484375\n",
      "epc:0,step:531,loss:1.406602,acc:0.515625\n",
      "epc:0,step:532,loss:1.353351,acc:0.437500\n",
      "epc:0,step:533,loss:1.249696,acc:0.531250\n",
      "epc:0,step:534,loss:1.545302,acc:0.390625\n",
      "epc:0,step:535,loss:1.120372,acc:0.609375\n",
      "epc:0,step:536,loss:1.304143,acc:0.468750\n",
      "epc:0,step:537,loss:1.335503,acc:0.500000\n",
      "epc:0,step:538,loss:1.220432,acc:0.562500\n",
      "epc:0,step:539,loss:1.250852,acc:0.484375\n",
      "epc:0,step:540,loss:1.543776,acc:0.468750\n",
      "epc:0,step:541,loss:1.408459,acc:0.500000\n",
      "epc:0,step:542,loss:1.257663,acc:0.484375\n",
      "epc:0,step:543,loss:1.330933,acc:0.562500\n",
      "epc:0,step:544,loss:1.579598,acc:0.406250\n",
      "epc:0,step:545,loss:1.477967,acc:0.437500\n",
      "epc:0,step:546,loss:1.373795,acc:0.484375\n",
      "epc:0,step:547,loss:1.379848,acc:0.484375\n",
      "epc:0,step:548,loss:1.582865,acc:0.421875\n",
      "epc:0,step:549,loss:1.451250,acc:0.468750\n",
      "epc:0,step:550,loss:1.507434,acc:0.406250\n",
      "epc:0,step:551,loss:1.330574,acc:0.500000\n",
      "epc:0,step:552,loss:1.362080,acc:0.562500\n",
      "epc:0,step:553,loss:1.454383,acc:0.421875\n",
      "epc:0,step:554,loss:1.428987,acc:0.421875\n",
      "epc:0,step:555,loss:1.296118,acc:0.593750\n",
      "epc:0,step:556,loss:1.235835,acc:0.531250\n",
      "epc:0,step:557,loss:1.398552,acc:0.453125\n",
      "epc:0,step:558,loss:1.249795,acc:0.562500\n",
      "epc:0,step:559,loss:1.258547,acc:0.468750\n",
      "epc:0,step:560,loss:1.294459,acc:0.531250\n",
      "epc:0,step:561,loss:1.367622,acc:0.484375\n",
      "epc:0,step:562,loss:1.482555,acc:0.343750\n",
      "epc:0,step:563,loss:1.579031,acc:0.390625\n",
      "epc:0,step:564,loss:1.321928,acc:0.515625\n",
      "epc:0,step:565,loss:1.453560,acc:0.453125\n",
      "epc:0,step:566,loss:1.421882,acc:0.375000\n",
      "epc:0,step:567,loss:1.601711,acc:0.390625\n",
      "epc:0,step:568,loss:1.359389,acc:0.546875\n",
      "epc:0,step:569,loss:1.262323,acc:0.515625\n",
      "epc:0,step:570,loss:1.503747,acc:0.421875\n",
      "epc:0,step:571,loss:1.198044,acc:0.500000\n",
      "epc:0,step:572,loss:1.386955,acc:0.468750\n",
      "epc:0,step:573,loss:1.199697,acc:0.515625\n",
      "epc:0,step:574,loss:1.451629,acc:0.468750\n",
      "epc:0,step:575,loss:1.439349,acc:0.500000\n",
      "epc:0,step:576,loss:1.201665,acc:0.546875\n",
      "epc:0,step:577,loss:1.326127,acc:0.484375\n",
      "epc:0,step:578,loss:1.267201,acc:0.593750\n",
      "epc:0,step:579,loss:1.357279,acc:0.531250\n",
      "epc:0,step:580,loss:1.426254,acc:0.593750\n",
      "epc:0,step:581,loss:1.859319,acc:0.312500\n",
      "epc:0,step:582,loss:1.268680,acc:0.500000\n",
      "epc:0,step:583,loss:1.221987,acc:0.546875\n",
      "epc:0,step:584,loss:1.694791,acc:0.375000\n",
      "epc:0,step:585,loss:1.335682,acc:0.562500\n",
      "epc:0,step:586,loss:1.084414,acc:0.656250\n",
      "epc:0,step:587,loss:1.355225,acc:0.437500\n",
      "epc:0,step:588,loss:1.314914,acc:0.531250\n",
      "epc:0,step:589,loss:1.352125,acc:0.468750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epc:0,step:590,loss:1.577927,acc:0.437500\n",
      "epc:0,step:591,loss:1.106823,acc:0.531250\n",
      "epc:0,step:592,loss:1.330951,acc:0.468750\n",
      "epc:0,step:593,loss:1.790042,acc:0.296875\n",
      "epc:0,step:594,loss:1.723180,acc:0.390625\n",
      "epc:0,step:595,loss:1.418445,acc:0.453125\n",
      "epc:0,step:596,loss:1.245846,acc:0.593750\n",
      "epc:0,step:597,loss:1.162270,acc:0.609375\n",
      "epc:0,step:598,loss:1.195924,acc:0.625000\n",
      "epc:0,step:599,loss:1.249360,acc:0.546875\n",
      "epc:0,step:600,loss:1.343473,acc:0.500000\n",
      "epc:0,step:601,loss:1.241715,acc:0.562500\n",
      "epc:0,step:602,loss:1.226144,acc:0.562500\n",
      "epc:0,step:603,loss:1.206354,acc:0.609375\n",
      "epc:0,step:604,loss:1.270456,acc:0.515625\n",
      "epc:0,step:605,loss:1.171633,acc:0.515625\n",
      "epc:0,step:606,loss:1.401724,acc:0.531250\n",
      "epc:0,step:607,loss:1.503014,acc:0.453125\n",
      "epc:0,step:608,loss:1.080057,acc:0.625000\n",
      "epc:0,step:609,loss:1.252995,acc:0.531250\n",
      "epc:0,step:610,loss:1.328882,acc:0.531250\n",
      "epc:0,step:611,loss:1.257773,acc:0.546875\n",
      "epc:0,step:612,loss:1.050150,acc:0.578125\n",
      "epc:0,step:613,loss:1.541835,acc:0.437500\n",
      "epc:0,step:614,loss:1.457378,acc:0.484375\n",
      "epc:0,step:615,loss:1.250440,acc:0.437500\n",
      "epc:0,step:616,loss:1.279203,acc:0.500000\n",
      "epc:0,step:617,loss:1.384503,acc:0.500000\n",
      "epc:0,step:618,loss:1.059081,acc:0.625000\n",
      "epc:0,step:619,loss:1.425198,acc:0.515625\n",
      "epc:0,step:620,loss:1.244993,acc:0.546875\n",
      "epc:0,step:621,loss:1.430833,acc:0.468750\n",
      "epc:0,step:622,loss:1.129067,acc:0.609375\n",
      "epc:0,step:623,loss:1.619503,acc:0.437500\n",
      "epc:0,step:624,loss:1.293459,acc:0.500000\n",
      "epc:0,step:625,loss:1.418012,acc:0.546875\n",
      "epc:0,step:626,loss:1.429347,acc:0.562500\n",
      "epc:0,step:627,loss:1.510039,acc:0.500000\n",
      "epc:0,step:628,loss:1.663743,acc:0.390625\n",
      "epc:0,step:629,loss:1.282148,acc:0.468750\n",
      "epc:0,step:630,loss:1.478513,acc:0.484375\n",
      "epc:0,step:631,loss:1.467508,acc:0.484375\n",
      "epc:0,step:632,loss:1.362193,acc:0.468750\n",
      "epc:0,step:633,loss:1.332334,acc:0.546875\n",
      "epc:0,step:634,loss:1.343378,acc:0.531250\n",
      "epc:0,step:635,loss:1.387449,acc:0.484375\n",
      "epc:0,step:636,loss:1.367874,acc:0.484375\n",
      "epc:0,step:637,loss:1.403047,acc:0.468750\n",
      "epc:0,step:638,loss:1.350506,acc:0.468750\n",
      "epc:0,step:639,loss:1.267218,acc:0.515625\n",
      "epc:0,step:640,loss:1.495743,acc:0.406250\n",
      "epc:0,step:641,loss:1.389536,acc:0.468750\n",
      "epc:0,step:642,loss:1.157897,acc:0.593750\n",
      "epc:0,step:643,loss:1.431021,acc:0.421875\n",
      "epc:0,step:644,loss:1.275400,acc:0.531250\n",
      "epc:0,step:645,loss:1.359324,acc:0.500000\n",
      "epc:0,step:646,loss:1.562539,acc:0.359375\n",
      "epc:0,step:647,loss:1.410192,acc:0.406250\n",
      "epc:0,step:648,loss:1.423728,acc:0.484375\n",
      "epc:0,step:649,loss:1.325454,acc:0.515625\n",
      "epc:0,step:650,loss:1.100549,acc:0.625000\n",
      "epc:0,step:651,loss:1.510759,acc:0.437500\n",
      "epc:0,step:652,loss:1.673671,acc:0.531250\n",
      "epc:0,step:653,loss:1.340000,acc:0.578125\n",
      "epc:0,step:654,loss:1.397800,acc:0.484375\n",
      "epc:0,step:655,loss:1.278756,acc:0.500000\n",
      "epc:0,step:656,loss:1.340387,acc:0.562500\n",
      "epc:0,step:657,loss:1.290807,acc:0.484375\n",
      "epc:0,step:658,loss:1.452892,acc:0.468750\n",
      "epc:0,step:659,loss:1.279821,acc:0.531250\n",
      "epc:0,step:660,loss:1.401400,acc:0.453125\n",
      "epc:0,step:661,loss:1.381211,acc:0.484375\n",
      "epc:0,step:662,loss:1.551010,acc:0.468750\n",
      "epc:0,step:663,loss:1.352357,acc:0.531250\n",
      "epc:0,step:664,loss:1.381943,acc:0.484375\n",
      "epc:0,step:665,loss:1.274923,acc:0.546875\n",
      "epc:0,step:666,loss:1.392513,acc:0.531250\n",
      "epc:0,step:667,loss:1.444659,acc:0.484375\n",
      "epc:0,step:668,loss:1.295054,acc:0.593750\n",
      "epc:0,step:669,loss:1.247672,acc:0.515625\n",
      "epc:0,step:670,loss:1.344896,acc:0.500000\n",
      "epc:0,step:671,loss:1.429326,acc:0.484375\n",
      "epc:0,step:672,loss:1.486205,acc:0.500000\n",
      "epc:0,step:673,loss:1.398777,acc:0.468750\n",
      "epc:0,step:674,loss:1.475263,acc:0.437500\n",
      "epc:0,step:675,loss:1.349089,acc:0.531250\n",
      "epc:0,step:676,loss:1.244335,acc:0.593750\n",
      "epc:0,step:677,loss:1.604342,acc:0.437500\n",
      "epc:0,step:678,loss:1.165676,acc:0.546875\n",
      "epc:0,step:679,loss:1.167775,acc:0.515625\n",
      "epc:0,step:680,loss:1.117496,acc:0.578125\n",
      "epc:0,step:681,loss:1.537572,acc:0.468750\n",
      "epc:0,step:682,loss:1.132017,acc:0.546875\n",
      "epc:0,step:683,loss:1.098751,acc:0.625000\n",
      "epc:0,step:684,loss:1.260929,acc:0.578125\n",
      "epc:0,step:685,loss:1.208152,acc:0.593750\n",
      "epc:0,step:686,loss:1.276356,acc:0.500000\n",
      "epc:0,step:687,loss:1.411224,acc:0.453125\n",
      "epc:0,step:688,loss:1.324657,acc:0.484375\n",
      "epc:0,step:689,loss:1.380038,acc:0.484375\n",
      "epc:0,step:690,loss:1.120300,acc:0.593750\n",
      "epc:0,step:691,loss:1.399555,acc:0.453125\n",
      "epc:0,step:692,loss:1.251184,acc:0.578125\n",
      "epc:0,step:693,loss:1.466823,acc:0.453125\n",
      "epc:0,step:694,loss:1.297854,acc:0.546875\n",
      "epc:0,step:695,loss:1.240839,acc:0.578125\n",
      "epc:0,step:696,loss:1.090526,acc:0.593750\n",
      "epc:0,step:697,loss:1.049294,acc:0.640625\n",
      "epc:0,step:698,loss:1.214964,acc:0.562500\n",
      "epc:0,step:699,loss:1.437222,acc:0.437500\n",
      "epc:0,step:700,loss:1.500644,acc:0.515625\n",
      "epc:0,step:701,loss:1.497132,acc:0.421875\n",
      "epc:0,step:702,loss:1.281334,acc:0.578125\n",
      "epc:0,step:703,loss:1.169476,acc:0.593750\n",
      "epc:0,step:704,loss:1.188987,acc:0.515625\n",
      "epc:0,step:705,loss:1.338086,acc:0.515625\n",
      "epc:0,step:706,loss:1.313414,acc:0.500000\n",
      "epc:0,step:707,loss:1.360245,acc:0.484375\n",
      "epc:0,step:708,loss:1.062916,acc:0.593750\n",
      "epc:0,step:709,loss:1.257369,acc:0.546875\n",
      "epc:0,step:710,loss:1.126928,acc:0.625000\n",
      "epc:0,step:711,loss:1.169734,acc:0.515625\n",
      "epc:0,step:712,loss:1.172139,acc:0.593750\n",
      "epc:0,step:713,loss:1.274696,acc:0.484375\n",
      "epc:0,step:714,loss:1.341440,acc:0.453125\n",
      "epc:0,step:715,loss:1.353299,acc:0.531250\n",
      "epc:0,step:716,loss:1.445505,acc:0.578125\n",
      "epc:0,step:717,loss:1.185972,acc:0.640625\n",
      "epc:0,step:718,loss:1.230143,acc:0.593750\n",
      "epc:0,step:719,loss:1.401591,acc:0.546875\n",
      "epc:0,step:720,loss:1.150714,acc:0.546875\n",
      "epc:0,step:721,loss:1.362413,acc:0.546875\n",
      "epc:0,step:722,loss:1.178100,acc:0.546875\n",
      "epc:0,step:723,loss:1.386785,acc:0.484375\n",
      "epc:0,step:724,loss:1.284945,acc:0.625000\n",
      "epc:0,step:725,loss:1.103884,acc:0.671875\n",
      "epc:0,step:726,loss:1.107542,acc:0.562500\n",
      "epc:0,step:727,loss:1.255611,acc:0.562500\n",
      "epc:0,step:728,loss:1.265864,acc:0.562500\n",
      "epc:0,step:729,loss:1.277536,acc:0.578125\n",
      "epc:0,step:730,loss:1.236683,acc:0.484375\n",
      "epc:0,step:731,loss:1.318506,acc:0.531250\n",
      "epc:0,step:732,loss:1.155117,acc:0.546875\n",
      "epc:0,step:733,loss:1.275007,acc:0.546875\n",
      "epc:0,step:734,loss:1.227367,acc:0.562500\n",
      "epc:0,step:735,loss:1.127836,acc:0.531250\n",
      "epc:0,step:736,loss:1.143727,acc:0.593750\n",
      "epc:0,step:737,loss:1.466827,acc:0.484375\n",
      "epc:0,step:738,loss:1.129016,acc:0.562500\n",
      "epc:0,step:739,loss:1.192947,acc:0.625000\n",
      "epc:0,step:740,loss:1.114757,acc:0.578125\n",
      "epc:0,step:741,loss:1.284138,acc:0.500000\n",
      "epc:0,step:742,loss:1.655843,acc:0.484375\n",
      "epc:0,step:743,loss:1.448496,acc:0.437500\n",
      "epc:0,step:744,loss:1.349965,acc:0.468750\n",
      "epc:0,step:745,loss:1.241386,acc:0.593750\n",
      "epc:0,step:746,loss:1.074540,acc:0.625000\n",
      "epc:0,step:747,loss:1.476521,acc:0.437500\n",
      "epc:0,step:748,loss:1.551446,acc:0.453125\n",
      "epc:0,step:749,loss:1.153787,acc:0.562500\n",
      "epc:0,step:750,loss:1.232000,acc:0.562500\n",
      "epc:0,step:751,loss:1.321814,acc:0.562500\n",
      "epc:0,step:752,loss:1.406005,acc:0.484375\n",
      "epc:0,step:753,loss:1.096412,acc:0.562500\n",
      "epc:0,step:754,loss:1.282475,acc:0.562500\n",
      "epc:0,step:755,loss:1.389928,acc:0.500000\n",
      "epc:0,step:756,loss:1.132697,acc:0.625000\n",
      "epc:0,step:757,loss:1.495786,acc:0.421875\n",
      "epc:0,step:758,loss:1.549482,acc:0.484375\n",
      "epc:0,step:759,loss:1.375373,acc:0.500000\n",
      "epc:0,step:760,loss:1.370753,acc:0.484375\n",
      "epc:0,step:761,loss:1.196062,acc:0.546875\n",
      "epc:0,step:762,loss:1.260141,acc:0.468750\n",
      "epc:0,step:763,loss:1.524340,acc:0.421875\n",
      "epc:0,step:764,loss:1.299474,acc:0.500000\n",
      "epc:0,step:765,loss:1.212029,acc:0.578125\n",
      "epc:0,step:766,loss:1.293602,acc:0.484375\n",
      "epc:0,step:767,loss:1.276585,acc:0.515625\n",
      "epc:0,step:768,loss:1.450209,acc:0.484375\n",
      "epc:0,step:769,loss:1.357944,acc:0.437500\n",
      "epc:0,step:770,loss:1.313786,acc:0.578125\n",
      "epc:0,step:771,loss:1.418468,acc:0.468750\n",
      "epc:0,step:772,loss:1.133949,acc:0.531250\n",
      "epc:0,step:773,loss:1.210734,acc:0.484375\n",
      "epc:0,step:774,loss:1.338585,acc:0.593750\n",
      "epc:0,step:775,loss:1.354394,acc:0.500000\n",
      "epc:0,step:776,loss:1.233558,acc:0.578125\n",
      "epc:0,step:777,loss:1.085552,acc:0.671875\n",
      "epc:0,step:778,loss:1.341776,acc:0.500000\n",
      "epc:0,step:779,loss:1.433788,acc:0.484375\n",
      "epc:0,step:780,loss:0.947191,acc:0.609375\n",
      "epc:0,step:781,loss:0.946205,acc:0.625000\n",
      "\n",
      "VAL set: Average loss: 0.0209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "save = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
    "net.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        if gpu_en:\n",
    "            inputs = inputs.to(device=gpus[0])## \n",
    "            labels = labels.to(device=gpus[0])\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred = torch.max(outputs, 1)[1]\n",
    "        acc = float(sum(np.array((y_pred == labels).cpu().numpy()))) / len(labels)\n",
    "        print('epc:%d,step:%d,loss:%f,acc:%f' % (epoch,i,loss,acc))\n",
    "        \n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in test_dataloader:\n",
    "        if gpu_en:\n",
    "            inputs = inputs.to(device=gpus[0])## \n",
    "            labels = labels.to(device=gpus[0])\n",
    "            outputs = net(inputs)\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    print('\\nVAL set: Average loss: {:.4f}\\n'.format(\n",
    "        test_loss))\n",
    "\n",
    "if save:\n",
    "    torch.save(net.state_dict(), 'net.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
